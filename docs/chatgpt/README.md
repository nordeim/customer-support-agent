# üß† Customer Support AI Agent  
*A friendly, smart, and knowledge-powered chatbot for customer support scenarios.*

[![Build Status](https://img.shields.io/github/actions/workflow/status/your-org/support-ai/ci.yml?branch=main)](https://github.com/your-org/support-ai/actions)  
[![Version](https://img.shields.io/github/v/tag/your-org/support-ai)](https://github.com/your-org/support-ai/releases)  
[![License](https://img.shields.io/github/license/your-org/support-ai)](LICENSE)  
[![Coverage](https://img.shields.io/codecov/c/github/your-org/support-ai)](https://codecov.io/gh/your-org/support-ai)  

## Table of Contents  
- [Introduction](#introduction)  
- [Technology Stack](#technology-stack)  
- [Architecture](#architecture)  
- [Quick Start](#quick-start)  
- [Deployment](#deployment)  
- [API Documentation](#api-documentation)  
- [Configuration](#configuration)  
- [Monitoring & Maintenance](#monitoring-maintenance)  
- [Contributing](#contributing)  
- [License & Acknowledgments](#license-acknowledgments)  

---

## Introduction  
### Project Overview and Purpose  
The Customer Support AI Agent is a production-ready chatbot solution designed to assist customers, answer questions from a support knowledge base, handle attachments, and seamlessly escalate to human agents when needed. It combines advanced AI-agent frameworks, retrieval-augmented generation (RAG), memory persistence, and a friendly front-end chatbot UI.

### Key Features & Capabilities  
- Conversational chat interface for customer support.  
- Persistent memory of threads and sessions via SQLite.  
- Knowledge-base powered responses using document embeddings and vector search (Chroma + EmbeddingGemma-300m).  
- Attachment ingestion (PDFs, docs, images) and conversion to searchable content.  
- Integration with the Microsoft Agent Framework (Python) for tool-based agent workflows, streaming responses, and human-in-the-loop escalation.  
- Observability: structured JSON logging, Prometheus metrics, health endpoints.  
- Dockerised and CI-gated for reliable deployment.

### Business Value & Use Cases  
- Automate first-level customer support, reducing human agent load.  
- Provide consistent, documented answers from internal knowledge bases.  
- Handle attachments from customers (screenshots, logs, PDFs) and extract insights.  
- Escalate to human agents when necessary, enhancing customer experience.  
- Deliver measurable SLAs: faster responses, lower cost, improved customer satisfaction.

### High-Level Architecture Summary  
The system consists of a React frontend chat UI, a FastAPI Python backend powered by the Microsoft Agent Framework, a lightweight SQLite memory store, and a Chroma vector store for document retrieval. The backend orchestrates conversation state, memory, tools, and knowledge retrieval to deliver informed responses.

---

## Technology Stack  
### Frontend  
- React (TypeScript) chat application  
- WebSocket/REST interface to backend  
- File-upload component for attachments  

### Backend  
- Python 3.11, FastAPI  
- Agent orchestration via Microsoft Agent Framework (Python) :contentReference[oaicite:0]{index=0}  
- Tooling layer for memory, RAG retrieval, attachment processing  

### Database & Storage  
- SQLite for session and memory persistence (zero-ops, file-based)  
- Chroma vector database for embeddings and retrieval of knowledge base content  

### AI/ML Components  
- Embedding model: `google/embeddinggemma-300m` (via Hugging Face)  
- RAG retrieval pipeline: embeddings ‚Üí Chroma ‚Üí agent input  
- Microsoft Agent Framework for chat & tool workflows :contentReference[oaicite:1]{index=1}  

### Monitoring & Observability  
- Prometheus metrics (`/metrics` endpoint)  
- Structured JSON logging with request-IDs  
- Health endpoint (`/healthz`)  
- CI pipeline: linting, type checking (mypy), unit/integration tests, coverage  

---

## Architecture  
### File Hierarchy  
```

/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                # FastAPI entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Settings & environment variables
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging_config.py      # Structured logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py             # Prometheus metrics setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sqlite.py          # SQLAlchemy engine
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py          # SQLite ORM models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_tool.py     # Session/memory persistence
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_tool.py        # Retrieval tool
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ attachment_tool.py # Attachment processing
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ escalation_tool.py # Escalation to human agent
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.py    # Document chunking & embedding prep
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ indexer.py         # Chroma indexing & query
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agents.py              # Agent setup & chat endpoint
‚îÇ   ‚îú‚îÄ‚îÄ migrations/                # DB migration scripts
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Container build
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml         # Local orchestration
‚îú‚îÄ‚îÄ frontend/                      # React chat UI
‚îî‚îÄ‚îÄ README.md                      # This documentation

````

### Interaction Diagram  
```mermaid
sequenceDiagram
  participant U as User  
  participant FE as Frontend (React)  
  participant BE as Backend (FastAPI)  
  participant KB as VectorDB (Chroma)  
  U->>FE: Sends chat message  
  FE->>BE: HTTP POST /chat/message  
  BE->>BE: Persist user message ‚Üí retrieve memory  
  BE->>KB: Query relevant docs via embeddings  
  KB-->>BE: Return top K docs  
  BE->>Agent: Construct prompt with memory + docs  
  Agent-->>BE: Generate response (stream or non-stream)  
  BE->>BE: Persist agent response  
  BE-->>FE: Return or stream response  
  FE-->>U: Display response  
````

### Application Logic Flow

```mermaid
flowchart TD
    A[Receive request /chat/message] --> B[Load session & memory]
    B --> C{Is stream flag set?}
    C -->|Yes| D[Retrieve docs via RAG]
    D --> E[Invoke agent.run_stream(...) for streaming output]
    E --> F[Persist response chunks]
    F --> G[Return SSE stream to frontend]
    C -->|No| H[Retrieve docs via RAG]
    H --> I[Invoke agent.run(...) for full response]
    I --> J[Persist response]
    J --> K[Return JSON response]
    K --> L{Response suggests escalation?}
    L -->|Yes| M[Trigger escalation tool ‚Üí create ticket]
    L -->|No| N[End]
```

### Component Relationships

* **MemoryTool**: Reads/writes conversation and memory entries to SQLite.
* **RAGTool**: Coordinates embeddings and Chroma vector search.
* **AttachmentTool**: Parses uploads and creates searchable content.
* **EscalationTool**: Handles triggering hand-off to human.
* **Agent (via Microsoft Agent Framework)**: Coordinates chat, tools, streaming.
* **Frontend**: Chat UI interacts with backend API.

---

## Quick Start

### Prerequisites

* Docker & Docker Compose installed
* Node.js & npm (for frontend)
* Python 3.11+
* Git repository cloned

### Installation Steps

1. Clone the repository:

   ```bash
   git clone https://github.com/your-org/support-ai.git
   cd support-ai
   ```
2. Navigate to backend and install dependencies:

   ```bash
   cd backend
   pip install -r requirements.txt
   ```
3. Build and start services (backend + vector DB placeholder):

   ```bash
   docker-compose up --build
   ```

### Configuration

Create a `.env` file in `backend/` with required variables:

```
OPENAI_API_KEY=your-key
OPENAI_CHAT_MODEL_ID=gpt-4o-mini
SQLITE_URL=sqlite:///./data/agent_memory.db
CHROMA_PERSIST_DIRECTORY=./data/chroma_db
CHROMA_COLLECTION_NAME=support_kb
ENVIRONMENT=development
```

### Running the Application

* Backend will be available at `http://localhost:8000`.
* Health check: `http://localhost:8000/healthz` ‚Üí `{ "status": "ok" }`
* Metrics: `http://localhost:8000/metrics`

### Basic Usage Example

```bash
curl -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{ "session_id": "1234", "message": "How do I return a damaged product?" }'
```

Response:

```json
{
  "session_id": "1234",
  "response": "You can return a damaged product within 30 days‚Ä¶ [reference source]"
}
```

---

## Deployment

### Development Deployment

* Use `docker-compose up` as above for local dev.
* Run tests:

  ```bash
  pytest --cov=backend/app
  ```
* Ensure linting & typing:

  ```bash
  black --check backend/app
  flake8 backend/app
  mypy backend/app
  ```

### Production Deployment

* Build and push Docker image, tag with version.
* Use orchestration platform (Kubernetes, ECS, etc) with volume mounts for SQLite & Chroma persistence.
* Ensure environment variables are injected via secure vault.
* Scale as needed: here are considerations‚Ä¶

  * **Scaling memory DB**: SQLite is file-based; for high concurrency consider PostgreSQL.
  * **Vector DB**: Chroma supports persistence but may require clustering; for large scale consider Weaviate or Pinecone.
  * **Stateless backend**: Keep containers stateless (persist only memory DB and vector store externally).
  * **Streaming mode**: If front-end uses SSE / WebSocket, ensure load-balancer supports sticky sessions or handle reconnects.

### Environment Variables (major ones)

| Name                       | Description                                         |
| -------------------------- | --------------------------------------------------- |
| `OPENAI_API_KEY`           | API key for OpenAI service                          |
| `OPENAI_CHAT_MODEL_ID`     | Model ID (e.g., `gpt-4o-mini`)                      |
| `SQLITE_URL`               | Connection string for SQLite memory store           |
| `CHROMA_PERSIST_DIRECTORY` | File‚Äêpath for Chroma vector persistence             |
| `CHROMA_COLLECTION_NAME`   | Name of the vector collection in Chroma             |
| `ENVIRONMENT`              | Deployment environment (`development`/`production`) |

---

## API Documentation

### Authentication

The API currently uses session IDs to track user threads. API keys or JWTs for user authentication can be added in future releases.

### Endpoints

| Method | Path            | Description                      | Request Body                                                 | Response Body                                |
| ------ | --------------- | -------------------------------- | ------------------------------------------------------------ | -------------------------------------------- |
| POST   | `/chat/message` | Send a user message to the agent | `{ "session_id": "...", "message": "...", "stream": false }` | `{ "session_id": "...", "response": "..." }` |
| GET    | `/healthz`      | Health check                     | ‚Äî                                                            | `{ "status": "ok" }`                         |
| GET    | `/metrics`      | Prometheus metrics endpoint      | ‚Äî                                                            | Plain text metrics                           |

### Streaming Example

```bash
curl -N -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{ "session_id": "1234", "message": "Tell me about return policy", "stream": true }'
```

The response will be a Server-Sent Events (SSE) stream with incremental `data:` chunks followed by `[DONE]`.

### Error Handling

* 400 Bad Request: malformed input.
* 500 Internal Server Error: agent or tool failure (check logs).
* Example response:

```json
{
  "detail": "Internal server error"
}
```

### Rate Limiting

Currently not enforced in this version. A future release may introduce rate-limiting middleware in the agent framework.

---

## Configuration

### Environment Variables

Refer to the table above in the *Deployment* section.

### Configuration Files

* `backend/app/config.py`: loads and validates settings.
* `docker-compose.yml`: orchestrates backend + vectordb placeholder.
* `migrations/run_migrations.sh`: idempotent SQLite schema.

### Customization Options

* Change embedding model (`EMBEDDING_MODEL_ID`) for higher fidelity.
* Tune chunk size (`KB_CHUNK_SIZE_SENTENCES`) for knowledge-base ingestion.
* Adjust retrieval top-K (`KB_TOP_K`) for balancing performance vs relevance.
* Switch memory DB to PostgreSQL by replacing SQLite dialect.

### Security Settings

* Ensure secrets (API keys) are not in version control.
* Validate attachments via `AttachmentTool` and sanitize stored data.
* Monitor tools usage (`agent_tool_invocations_total`) for anomalous activity.

---

## Monitoring & Maintenance

### Health Checks

* `/healthz`: basic status endpoint.
* Metrics pipeline: `/metrics` (Prometheus).

### Logging

* Structured JSON logging with `request_id` to correlate sessions.
* Log levels configurable via `LOG_LEVEL` environment variable.

### Metrics

* `agent_request_count_total` ‚Äì total chat requests.
* `agent_request_latency_seconds` ‚Äì latency of non-stream requests.
* `agent_stream_request_latency_seconds` ‚Äì latency of streaming responses.
* `agent_tool_invocations_total` ‚Äì number of tool calls (memory, RAG, attachments).

### Troubleshooting

* *Slow responses*: Investigate model latency or vector store performance.
* *‚Äúdatabase locked‚Äù error*: SQLite concurrency limit ‚Äî consider switching to PostgreSQL.
* *Zero results from RAG*: Check embeddings generation and Chroma persistence.
* *Agent keeps escalating unnecessarily*: Adjust prompt or tool logic in `EscalationTool`.

### Backup & Recovery

* Backup SQLite `.db` file periodically.
* Backup Chroma persistence directory (`CHROMA_PERSIST_DIRECTORY`).
* Rollback script: `deploy/rollback.sh` (see runbook)

---

## Contributing

### Development Setup

```bash
git clone https://github.com/your-org/support-ai.git
cd support-ai/backend
pip install -r requirements.txt
```

### Code Style Guidelines

* Follows `black`, `flake8`, `mypy` for formatting, linting, typing.
* Typing declarations mandatory for public APIs.
* Single responsibility per module, clear docstrings.

### Pull Request Process

1. Create a feature branch (`feature/your-feature`).
2. Write code, include unit + integration tests.
3. Ensure linting and tests pass locally.
4. Open a PR, link the issue, describe changes.
5. CI will run lint/tests.
6. After review and approval, merge into `main`.

### Testing Guidelines

* Unit tests: `tests/unit/` (fast, isolated).
* Integration tests: `tests/integration/` (covers real flow, slower).
* Use deterministic seeds where non-determinism exists (e.g., memory ordering).
* Coverage target: ‚â• 80%.

---

## License & Acknowledgments

**License:** [MIT](LICENSE)
**Contributors:** Thanks to all developers and reviewers who contributed to this version.
**Third-Party Attributions:**

* Microsoft Agent Framework (Python) ‚Äì Agent orchestration framework. ([Microsoft Learn][1])
* Embedding model: `google/embeddinggemma-300m` (Hugging Face)
* Vector store: Chroma (open source)
* Attachment processor: MarkItDown
* Frontend: React community libraries

**References:**

* Microsoft Agent Framework documentation ‚Äì [Microsoft Learn](https://learn.microsoft.com/en-us/agent-framework/)
* GitHub repo ‚Äì [microsoft/agent-framework](https://github.com/microsoft/agent-framework)

---

Thank you for exploring the Customer Support AI Agent. We look forward to your feedback and contributions!

[1]: https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-types/chat-client-agent?utm_source=chatgpt.com "Agent based on any IChatClient | Microsoft Learn"
