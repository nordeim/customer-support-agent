# Phase 4 â€” Deployment Automation and Monitoring

## Phase 4 Planning

Based on our successful completion of Phase 3, I'll now proceed with Phase 4, which focuses on deployment automation, comprehensive monitoring, and final handoff artifacts. This phase is critical for ensuring our Production-Ready system can be deployed, monitored, and maintained effectively in a production environment.

### Phase 4 Objectives

1. **Deployment Automation**: Create comprehensive deployment scripts and CI/CD pipelines
2. **Monitoring Dashboards**: Implement detailed monitoring dashboards and alerting
3. **Performance Optimization**: Optimize system performance and add caching
4. **Documentation**: Create comprehensive runbooks and deployment guides
5. **Handoff Artifacts**: Prepare all necessary artifacts for production deployment

### Sprint A: Deployment Automation

#### 1. Enhanced Docker Configuration

I'll create production-ready Docker configurations with multi-stage builds, security scanning, and optimization:

```dockerfile
# backend/Dockerfile.prod
# Multi-stage production Dockerfile
FROM python:3.11-slim as builder

# Set build arguments
ARG BUILD_DATE
ARG VCS_REF
ARG VERSION

# Set labels
LABEL maintainer="customer-support-team@example.com" \
      org.label-schema.build-date=$BUILD_DATE \
      org.label-schema.name="customer-support-backend" \
      org.label-schema.description="Customer Support AI Agent Backend" \
      org.label-schema.url="https://github.com/example/customer-support-agent" \
      org.label-schema.vcs-ref=$VCS_REF \
      org.label-schema.vcs-url="https://github.com/example/customer-support-agent" \
      org.label-schema.vendor="Customer Support Team" \
      org.label-schema.version=$VERSION \
      org.label-schema.schema-version="1.0"

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create app user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Set work directory
WORKDIR /app

# Copy requirements and install Python dependencies
COPY requirements.txt requirements-prod.txt ./
RUN pip install --user -r requirements-prod.txt

# Production stage
FROM python:3.11-slim as production

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PATH=/home/appuser/.local/bin:$PATH

# Install runtime dependencies
RUN apt-get update && apt-get install -y \
    libpq5 \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create app user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Set work directory
WORKDIR /app

# Copy Python packages from builder stage
COPY --from=builder /root/.local /home/appuser/.local

# Copy application code
COPY --chown=appuser:appuser . .

# Create necessary directories
RUN mkdir -p /app/uploads /app/data /app/logs && \
    chown -R appuser:appuser /app

# Switch to non-root user
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/liveness || exit 1

# Expose port
EXPOSE 8000

# Run the application
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

```dockerfile
# frontend/Dockerfile.prod
# Multi-stage production Dockerfile for frontend
FROM node:18-alpine as builder

# Set build arguments
ARG BUILD_DATE
ARG VCS_REF
ARG VERSION
ARG REACT_APP_API_URL

# Set labels
LABEL maintainer="customer-support-team@example.com" \
      org.label-schema.build-date=$BUILD_DATE \
      org.label-schema.name="customer-support-frontend" \
      org.label-schema.description="Customer Support AI Agent Frontend" \
      org.label-schema.url="https://github.com/example/customer-support-agent" \
      org.label-schema.vcs-ref=$VCS_REF \
      org.label-schema.vcs-url="https://github.com/example/customer-support-agent" \
      org.label-schema.vendor="Customer Support Team" \
      org.label-schema.version=$VERSION \
      org.label-schema.schema-version="1.0"

# Set work directory
WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production && npm cache clean --force

# Copy source code
COPY . .

# Set environment variables
ENV REACT_APP_API_URL=$REACT_APP_API_URL \
    GENERATE_SOURCEMAP=false \
    INLINE_RUNTIME_CHUNK=false

# Build the application
RUN npm run build

# Production stage
FROM nginx:alpine as production

# Install curl for health checks
RUN apk add --no-cache curl

# Copy custom nginx configuration
COPY nginx.prod.conf /etc/nginx/nginx.conf

# Copy built application
COPY --from=builder /app/build /usr/share/nginx/html

# Create non-root user
RUN addgroup -g 1001 -S nginx && \
    adduser -S nginx -u 1001

# Set proper permissions
RUN chown -R nginx:nginx /usr/share/nginx/html && \
    chown -R nginx:nginx /var/cache/nginx && \
    chown -R nginx:nginx /var/log/nginx && \
    chown -R nginx:nginx /etc/nginx/conf.d

# Create nginx runtime directories
RUN touch /var/run/nginx.pid && \
    chown -R nginx:nginx /var/run/nginx.pid

# Switch to non-root user
USER nginx

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:80/health || exit 1

# Expose port
EXPOSE 80

# Start nginx
CMD ["nginx", "-g", "daemon off;"]
```

#### 2. Production Docker Compose

```yaml
# docker-compose.prod.yml
version: "3.8"

services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.prod
      args:
        BUILD_DATE: ${BUILD_DATE}
        VCS_REF: ${VCS_REF}
        VERSION: ${VERSION}
    image: customer-support-backend:${VERSION}
    container_name: customer-support-backend
    restart: unless-stopped
    environment:
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      - AGENT_FRAMEWORK_ENDPOINT=${AGENT_FRAMEWORK_ENDPOINT}
      - AGENT_FRAMEWORK_API_KEY=${AGENT_FRAMEWORK_API_KEY}
      - SECRET_KEY=${SECRET_KEY}
      - LOG_LEVEL=INFO
      - PROMETHEUS_ENABLED=true
    volumes:
      - ./data/uploads:/app/uploads
      - ./data/logs:/app/logs
      - ./data/chroma:/app/chroma
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/liveness"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    networks:
      - customer-support-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
      args:
        BUILD_DATE: ${BUILD_DATE}
        VCS_REF: ${VCS_REF}
        VERSION: ${VERSION}
        REACT_APP_API_URL: ${REACT_APP_API_URL}
    image: customer-support-frontend:${VERSION}
    container_name: customer-support-frontend
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    networks:
      - customer-support-network

  postgres:
    image: postgres:15-alpine
    container_name: customer-support-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./postgres/init:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - customer-support-network

  redis:
    image: redis:7-alpine
    container_name: customer-support-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    networks:
      - customer-support-network

  prometheus:
    image: prom/prometheus:latest
    container_name: customer-support-prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    depends_on:
      backend:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - customer-support-network

  grafana:
    image: grafana/grafana:latest
    container_name: customer-support-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      - "3000:3000"
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - customer-support-network

  alertmanager:
    image: prom/alertmanager:latest
    container_name: customer-support-alertmanager
    restart: unless-stopped
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    ports:
      - "9093:9093"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - customer-support-network

  nginx-proxy:
    image: nginx:alpine
    container_name: customer-support-nginx-proxy
    restart: unless-stopped
    volumes:
      - ./nginx/nginx-proxy.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - frontend
      - backend
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - customer-support-network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  alertmanager_data:
    driver: local

networks:
  customer-support-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

#### 3. Environment Configuration

```bash
# .env.prod
# Build configuration
VERSION=1.0.0
BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
VCS_REF=$(git rev-parse --short HEAD)

# Database configuration
POSTGRES_DB=customer_support
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_secure_postgres_password

# Redis configuration
REDIS_PASSWORD=your_secure_redis_password

# Application configuration
SECRET_KEY=your_very_secure_secret_key_here
AGENT_FRAMEWORK_ENDPOINT=https://your-agent-framework-endpoint.com
AGENT_FRAMEWORK_API_KEY=your_agent_framework_api_key

# Frontend configuration
REACT_APP_API_URL=https://api.yourdomain.com

# Monitoring configuration
GRAFANA_USER=admin
GRAFANA_PASSWORD=your_secure_grafana_password

# SSL configuration
SSL_CERT_PATH=/etc/nginx/ssl/cert.pem
SSL_KEY_PATH=/etc/nginx/ssl/key.pem
```

#### 4. Deployment Scripts

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
ENV_FILE="${PROJECT_ROOT}/.env.prod"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.prod.yml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if required files exist
check_prerequisites() {
    log_info "Checking prerequisites..."
    
    if [ ! -f "$ENV_FILE" ]; then
        log_error "Environment file not found: $ENV_FILE"
        exit 1
    fi
    
    if [ ! -f "$COMPOSE_FILE" ]; then
        log_error "Docker Compose file not found: $COMPOSE_FILE"
        exit 1
    fi
    
    # Check if Docker is running
    if ! docker info > /dev/null 2>&1; then
        log_error "Docker is not running"
        exit 1
    fi
    
    # Check if Docker Compose is available
    if ! command -v docker-compose > /dev/null 2>&1; then
        log_error "Docker Compose is not installed"
        exit 1
    fi
    
    log_info "Prerequisites check passed"
}

# Load environment variables
load_environment() {
    log_info "Loading environment variables..."
    
    # Load environment variables from .env.prod
    set -a
    source "$ENV_FILE"
    set +a
    
    # Export required variables
    export VERSION BUILD_DATE VCS_REF
    export POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD
    export REDIS_PASSWORD
    export SECRET_KEY AGENT_FRAMEWORK_ENDPOINT AGENT_FRAMEWORK_API_KEY
    export REACT_APP_API_URL
    export GRAFANA_USER GRAFANA_PASSWORD
    
    log_info "Environment variables loaded"
}

# Build Docker images
build_images() {
    log_info "Building Docker images..."
    
    cd "$PROJECT_ROOT"
    
    # Build backend image
    log_info "Building backend image..."
    docker build -f backend/Dockerfile.prod -t customer-support-backend:$VERSION ./backend
    
    # Build frontend image
    log_info "Building frontend image..."
    docker build -f frontend/Dockerfile.prod -t customer-support-frontend:$VERSION ./frontend
    
    log_info "Docker images built successfully"
}

# Run database migrations
run_migrations() {
    log_info "Running database migrations..."
    
    cd "$PROJECT_ROOT"
    
    # Start only the database service
    docker-compose -f "$COMPOSE_FILE" up -d postgres
    
    # Wait for database to be ready
    log_info "Waiting for database to be ready..."
    sleep 10
    
    # Run migrations
    docker-compose -f "$COMPOSE_FILE" run --rm backend python scripts/migrate.py upgrade
    
    log_info "Database migrations completed"
}

# Deploy services
deploy_services() {
    log_info "Deploying services..."
    
    cd "$PROJECT_ROOT"
    
    # Stop existing services
    docker-compose -f "$COMPOSE_FILE" down
    
    # Start services
    docker-compose -f "$COMPOSE_FILE" up -d
    
    log_info "Services deployed"
}

# Wait for services to be healthy
wait_for_services() {
    log_info "Waiting for services to be healthy..."
    
    local services=("postgres" "redis" "backend" "frontend" "prometheus" "grafana")
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        local all_healthy=true
        
        for service in "${services[@]}"; do
            if ! docker-compose -f "$COMPOSE_FILE" ps "$service" | grep -q "Up (healthy)"; then
                all_healthy=false
                break
            fi
        done
        
        if [ "$all_healthy" = true ]; then
            log_info "All services are healthy"
            return 0
        fi
        
        log_info "Waiting for services... (attempt $attempt/$max_attempts)"
        sleep 10
        ((attempt++))
    done
    
    log_error "Services did not become healthy within the expected time"
    return 1
}

# Run health checks
run_health_checks() {
    log_info "Running health checks..."
    
    # Check backend health
    if curl -f http://localhost:8000/health > /dev/null 2>&1; then
        log_info "Backend health check passed"
    else
        log_error "Backend health check failed"
        return 1
    fi
    
    # Check frontend health
    if curl -f http://localhost:80/health > /dev/null 2>&1; then
        log_info "Frontend health check passed"
    else
        log_error "Frontend health check failed"
        return 1
    fi
    
    # Check Prometheus health
    if curl -f http://localhost:9090/-/healthy > /dev/null 2>&1; then
        log_info "Prometheus health check passed"
    else
        log_error "Prometheus health check failed"
        return 1
    fi
    
    # Check Grafana health
    if curl -f http://localhost:3000/api/health > /dev/null 2>&1; then
        log_info "Grafana health check passed"
    else
        log_error "Grafana health check failed"
        return 1
    fi
    
    log_info "All health checks passed"
}

# Cleanup old images
cleanup() {
    log_info "Cleaning up old Docker images..."
    
    # Remove unused images
    docker image prune -f
    
    # Remove old versions of our images (keep last 2 versions)
    docker images --format "table {{.Repository}}\t{{.Tag}}" | grep "customer-support" | \
    tail -n +3 | awk '{print $1":"$2}' | tail -n +3 | \
    xargs -r docker rmi
    
    log_info "Cleanup completed"
}

# Main deployment function
main() {
    log_info "Starting deployment..."
    
    check_prerequisites
    load_environment
    build_images
    run_migrations
    deploy_services
    wait_for_services
    run_health_checks
    cleanup
    
    log_info "Deployment completed successfully!"
    
    # Display service URLs
    echo ""
    log_info "Service URLs:"
    echo "  Frontend: http://localhost:80"
    echo "  Backend API: http://localhost:8000"
    echo "  Prometheus: http://localhost:9090"
    echo "  Grafana: http://localhost:3000"
    echo "  AlertManager: http://localhost:9093"
    echo ""
    log_info "Grafana credentials: $GRAFANA_USER / $GRAFANA_PASSWORD"
}

# Handle script arguments
case "${1:-deploy}" in
    "deploy")
        main
        ;;
    "build")
        check_prerequisites
        load_environment
        build_images
        ;;
    "migrate")
        check_prerequisites
        load_environment
        run_migrations
        ;;
    "health")
        run_health_checks
        ;;
    "cleanup")
        cleanup
        ;;
    "logs")
        docker-compose -f "$COMPOSE_FILE" logs -f "${2:-}"
        ;;
    "stop")
        docker-compose -f "$COMPOSE_FILE" down
        ;;
    "restart")
        docker-compose -f "$COMPOSE_FILE" restart
        ;;
    *)
        echo "Usage: $0 {deploy|build|migrate|health|cleanup|logs|stop|restart}"
        exit 1
        ;;
esac
```

#### 5. Rollback Script

```bash
#!/bin/bash
# scripts/rollback.sh

set -e

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
ENV_FILE="${PROJECT_ROOT}/.env.prod"
COMPOSE_FILE="${PROJECT_ROOT}/docker-compose.prod.yml"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging functions
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Load environment variables
load_environment() {
    log_info "Loading environment variables..."
    
    if [ ! -f "$ENV_FILE" ]; then
        log_error "Environment file not found: $ENV_FILE"
        exit 1
    fi
    
    # Load environment variables from .env.prod
    set -a
    source "$ENV_FILE"
    set +a
    
    log_info "Environment variables loaded"
}

# Get previous version
get_previous_version() {
    log_info "Getting previous version..."
    
    # Get the previous version tag
    local previous_version=$(git describe --tags --abbrev=0 HEAD~1 2>/dev/null || echo "")
    
    if [ -z "$previous_version" ]; then
        log_error "No previous version found"
        exit 1
    fi
    
    echo "$previous_version"
}

# Backup current data
backup_data() {
    log_info "Backing up current data..."
    
    local backup_dir="${PROJECT_ROOT}/backups/$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$backup_dir"
    
    # Backup database
    docker-compose -f "$COMPOSE_FILE" exec -T postgres pg_dump -U "$POSTGRES_USER" "$POSTGRES_DB" > "$backup_dir/database.sql"
    
    # Backup Redis data
    docker cp $(docker-compose -f "$COMPOSE_FILE" ps -q redis):/data "$backup_dir/redis"
    
    # Backup uploads
    docker cp $(docker-compose -f "$COMPOSE_FILE" ps -q backend):/app/uploads "$backup_dir/"
    
    log_info "Data backed up to: $backup_dir"
    echo "$backup_dir"
}

# Rollback to previous version
rollback_to_version() {
    local target_version="$1"
    
    log_info "Rolling back to version: $target_version"
    
    # Stop current services
    docker-compose -f "$COMPOSE_FILE" down
    
    # Pull previous images
    docker pull customer-support-backend:$target_version
    docker pull customer-support-frontend:$target_version
    
    # Update environment file with previous version
    sed -i.bak "s/VERSION=.*/VERSION=$target_version/" "$ENV_FILE"
    
    # Load updated environment
    load_environment
    
    # Start services with previous version
    docker-compose -f "$COMPOSE_FILE" up -d
    
    log_info "Rollback to version $target_version completed"
}

# Restore data if needed
restore_data() {
    local backup_dir="$1"
    
    if [ -z "$backup_dir" ] || [ ! -d "$backup_dir" ]; then
        log_warn "No backup directory provided or directory does not exist"
        return 0
    fi
    
    log_info "Restoring data from backup: $backup_dir"
    
    # Restore database
    if [ -f "$backup_dir/database.sql" ]; then
        docker-compose -f "$COMPOSE_FILE" exec -T postgres psql -U "$POSTGRES_USER" -d "$POSTGRES_DB" < "$backup_dir/database.sql"
        log_info "Database restored"
    fi
    
    # Restore Redis data
    if [ -d "$backup_dir/redis" ]; then
        docker cp "$backup_dir/redis" $(docker-compose -f "$COMPOSE_FILE" ps -q redis):/
        log_info "Redis data restored"
    fi
    
    # Restore uploads
    if [ -d "$backup_dir/uploads" ]; then
        docker cp "$backup_dir/uploads" $(docker-compose -f "$COMPOSE_FILE" ps -q backend):/app/
        log_info "Uploads restored"
    fi
    
    log_info "Data restoration completed"
}

# Verify rollback
verify_rollback() {
    log_info "Verifying rollback..."
    
    # Wait for services to be healthy
    local max_attempts=30
    local attempt=1
    
    while [ $attempt -le $max_attempts ]; do
        if curl -f http://localhost:8000/health > /dev/null 2>&1 && \
           curl -f http://localhost:80/health > /dev/null 2>&1; then
            log_info "Rollback verification successful"
            return 0
        fi
        
        log_info "Waiting for services... (attempt $attempt/$max_attempts)"
        sleep 10
        ((attempt++))
    done
    
    log_error "Rollback verification failed"
    return 1
}

# Main rollback function
main() {
    local target_version="${1:-}"
    local backup_dir=""
    
    # If no version specified, get previous version
    if [ -z "$target_version" ]; then
        target_version=$(get_previous_version)
    fi
    
    log_info "Starting rollback to version: $target_version"
    
    load_environment
    
    # Backup current data
    backup_dir=$(backup_data)
    
    # Perform rollback
    rollback_to_version "$target_version"
    
    # Wait for services
    sleep 30
    
    # Verify rollback
    if verify_rollback; then
        log_info "Rollback completed successfully!"
        
        # Ask if user wants to restore data
        read -p "Do you want to restore data from backup? (y/N): " -n 1 -r
        echo
        if [[ $REPLY =~ ^[Yy]$ ]]; then
            restore_data "$backup_dir"
        fi
    else
        log_error "Rollback verification failed!"
        log_info "You can restore data from backup manually: $backup_dir"
        exit 1
    fi
}

# Handle script arguments
case "${1:-}" in
    "")
        main
        ;;
    "to")
        main "$2"
        ;;
    "restore")
        load_environment
        restore_data "$2"
        ;;
    *)
        echo "Usage: $0 [to <version>] [restore <backup_dir>]"
        echo "Examples:"
        echo "  $0                    # Rollback to previous version"
        echo "  $0 to 1.0.0          # Rollback to specific version"
        echo "  $0 restore /path/to/backup  # Restore data from backup"
        exit 1
        ;;
esac
```

### Sprint B: Monitoring Dashboards

#### 1. Prometheus Configuration

```yaml
# monitoring/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'customer-support-backend'
    static_configs:
      - targets: ['backend:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s
    scrape_timeout: 10s

  - job_name: 'customer-support-frontend'
    static_configs:
      - targets: ['frontend:80']
    metrics_path: '/metrics'
    scrape_interval: 30s
    scrape_timeout: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']
    scrape_interval: 30s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
    scrape_interval: 30s

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

#### 2. Alerting Rules

```yaml
# monitoring/prometheus/rules/alerts.yml
groups:
  - name: customer-support-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(errors_total[5m]) / rate(chat_messages_total[5m]) * 100 > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }}% for the last 5 minutes"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(chat_message_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s"

      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Service is down"
          description: "{{ $labels.instance }} has been down for more than 1 minute"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}%"

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}%"

      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Low disk space"
          description: "Disk space is {{ $value }}% available"

      - alert: DatabaseConnectionsHigh
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "Database has {{ $value }} active connections"

      - alert: EscalationRateHigh
        expr: rate(chat_escalations_total[5m]) * 60 > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High escalation rate"
          description: "Escalation rate is {{ $value }} per minute"

      - alert: RAGQueryFailureRate
        expr: rate(rag_queries_total{status="error"}[5m]) / rate(rag_queries_total[5m]) * 100 > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High RAG query failure rate"
          description: "RAG query failure rate is {{ $value }}%"
```

#### 3. AlertManager Configuration

```yaml
# monitoring/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'your_smtp_password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
    - match:
        severity: warning
      receiver: 'warning-alerts'

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://127.0.0.1:5001/'

  - name: 'critical-alerts'
    email_configs:
      - to: 'team@example.com'
        subject: '[CRITICAL] Customer Support Alert'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
    slack_configs:
      - api_url: 'YOUR_SLACK_WEBHOOK_URL'
        channel: '#alerts'
        title: 'Critical Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'warning-alerts'
    email_configs:
      - to: 'team@example.com'
        subject: '[WARNING] Customer Support Alert'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

#### 4. Grafana Dashboard Configuration

```json
{
  "dashboard": {
    "id": null,
    "title": "Customer Support Overview",
    "tags": ["customer-support"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(chat_messages_total[5m])",
            "legendFormat": "{{role}}"
          }
        ],
        "yAxes": [
          {
            "label": "Requests per second"
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(chat_message_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(chat_message_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(chat_message_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ],
        "yAxes": [
          {
            "label": "Seconds"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(errors_total[5m]) / rate(chat_messages_total[5m]) * 100",
            "legendFormat": "Error Rate %"
          }
        ],
        "yAxes": [
          {
            "label": "Percentage",
            "max": 100,
            "min": 0
          }
        ]
      },
      {
        "id": 4,
        "title": "Active Sessions",
        "type": "stat",
        "targets": [
          {
            "expr": "active_sessions",
            "legendFormat": "Active Sessions"
          }
        ]
      },
      {
        "id": 5,
        "title": "Escalations",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(chat_escalations_total[5m]) * 60",
            "legendFormat": "Escalations per minute"
          }
        ]
      },
      {
        "id": 6,
        "title": "RAG Queries",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(rag_queries_total[5m])",
            "legendFormat": "RAG Queries per second"
          }
        ]
      },
      {
        "id": 7,
        "title": "Attachment Processing",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(attachment_processing_total[5m])",
            "legendFormat": "{{content_type}}"
          }
        ]
      },
      {
        "id": 8,
        "title": "System Health",
        "type": "table",
        "targets": [
          {
            "expr": "up",
            "legendFormat": "{{instance}}"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

#### 5. Grafana Provisioning

```yaml
# monitoring/grafana/provisioning/datasources/prometheus.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

```yaml
# monitoring/grafana/provisioning/dashboards/dashboard.yml
apiVersion: 1

providers:
  - name: 'customer-support-dashboards'
    orgId: 1
    folder: 'Customer Support'
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

### Sprint C: Performance Optimization

#### 1. Redis Caching Implementation

```python
# backend/app/core/cache.py
import redis
import json
import pickle
from typing import Any, Optional, Union
from datetime import timedelta
from .config import settings
from .logging import logger

class CacheManager:
    """Redis-based cache manager"""
    
    def __init__(self):
        self.redis_client = redis.Redis(
            host=settings.redis_host,
            port=settings.redis_port,
            password=settings.redis_password,
            db=0,
            decode_responses=False
        )
        
        # Test connection
        try:
            self.redis_client.ping()
            logger.info("Redis connection established")
        except Exception as e:
            logger.error(f"Failed to connect to Redis: {str(e)}")
            self.redis_client = None
    
    def is_available(self) -> bool:
        """Check if Redis is available"""
        return self.redis_client is not None
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
        """Set a value in cache"""
        if not self.is_available():
            return False
        
        try:
            # Serialize value
            serialized_value = pickle.dumps(value)
            
            # Set with TTL if provided
            if ttl:
                return self.redis_client.setex(key, ttl, serialized_value)
            else:
                return self.redis_client.set(key, serialized_value)
        except Exception as e:
            logger.error(f"Failed to set cache key {key}: {str(e)}")
            return False
    
    def get(self, key: str) -> Optional[Any]:
        """Get a value from cache"""
        if not self.is_available():
            return None
        
        try:
            value = self.redis_client.get(key)
            if value:
                return pickle.loads(value)
            return None
        except Exception as e:
            logger.error(f"Failed to get cache key {key}: {str(e)}")
            return None
    
    def delete(self, key: str) -> bool:
        """Delete a key from cache"""
        if not self.is_available():
            return False
        
        try:
            return bool(self.redis_client.delete(key))
        except Exception as e:
            logger.error(f"Failed to delete cache key {key}: {str(e)}")
            return False
    
    def exists(self, key: str) -> bool:
        """Check if a key exists in cache"""
        if not self.is_available():
            return False
        
        try:
            return bool(self.redis_client.exists(key))
        except Exception as e:
            logger.error(f"Failed to check cache key {key}: {str(e)}")
            return False
    
    def clear_pattern(self, pattern: str) -> int:
        """Clear keys matching a pattern"""
        if not self.is_available():
            return 0
        
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            logger.error(f"Failed to clear cache pattern {pattern}: {str(e)}")
            return 0

# Global cache manager instance
cache_manager = CacheManager()

def cache_result(key_prefix: str, ttl: int = 300):
    """Decorator to cache function results"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = f"{key_prefix}:{hash(str(args) + str(kwargs))}"
            
            # Try to get from cache
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Execute function and cache result
            result = func(*args, **kwargs)
            cache_manager.set(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator
```

#### 2. Enhanced RAG Tool with Caching

```python
# backend/app/tools/rag_tool.py
from typing import Dict, Any, List
import hashlib
from ..vector_store.chroma_client import ChromaClient
from ..vector_store.embeddings import EmbeddingModel
from ..core.cache import cache_manager, cache_result
from ..core.logging import logger
from ..core.config import settings
from ..metrics import track_rag_metrics

class RAGTool:
    """Tool for Retrieval-Augmented Generation with caching"""
    
    def __init__(self):
        self.chroma_client = ChromaClient(
            persist_directory=settings.chroma_persist_directory
        )
        self.embedding_model = EmbeddingModel(
            model_name=settings.embedding_model_name
        )
    
    def _generate_cache_key(self, query: str, n_results: int) -> str:
        """Generate cache key for RAG query"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return f"rag_query:{query_hash}:{n_results}"
    
    @track_rag_metrics
    def execute(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the RAG tool with caching"""
        action = parameters.get("action")
        
        if action == "search":
            return self._search_cached(parameters)
        elif action == "add_document":
            return self._add_document(parameters)
        else:
            return {"error": f"Unknown action: {action}"}
    
    def _search_cached(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Search with caching"""
        query = parameters.get("query")
        n_results = parameters.get("n_results", 5)
        session_id = parameters.get("session_id")
        
        if not query:
            return {"error": "Query is required"}
        
        # Generate cache key
        cache_key = self._generate_cache_key(query, n_results)
        
        # Try to get from cache
        cached_result = cache_manager.get(cache_key)
        if cached_result:
            logger.info(
                f"RAG cache hit for query: {query[:50]}...",
                extra={"session_id": session_id, "tool_name": "rag_tool"}
            )
            return cached_result
        
        # Perform search
        result = self._search(parameters)
        
        # Cache result for 30 minutes
        cache_manager.set(cache_key, result, ttl=1800)
        
        return result
    
    def _search(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Perform actual search"""
        query = parameters.get("query")
        n_results = parameters.get("n_results", 5)
        session_id = parameters.get("session_id")
        
        try:
            # Generate embedding for the query
            query_embedding = self.embedding_model.embed_query(query)
            
            # Search in Chroma
            results = self.chroma_client.query(
                query_embeddings=[query_embedding],
                n_results=n_results
            )
            
            # Format results
            documents = []
            for i, doc in enumerate(results["documents"][0]):
                documents.append({
                    "id": results["ids"][0][i],
                    "content": doc,
                    "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                    "distance": results["distances"][0][i] if "distances" in results else None
                })
            
            logger.info(
                f"RAG search for session {session_id} returned {len(documents)} documents",
                extra={"session_id": session_id, "tool_name": "rag_tool", "query": query}
            )
            
            return {"documents": documents}
        except Exception as e:
            logger.error(
                f"RAG search failed for session {session_id}: {str(e)}",
                extra={"session_id": session_id, "tool_name": "rag_tool"}
            )
            return {"error": f"Search failed: {str(e)}"}
    
    def _add_document(self, parameters: Dict[str, Any]) -> Dict[str, Any]:
        """Add a document to the vector store"""
        content = parameters.get("content")
        metadata = parameters.get("metadata", {})
        document_id = parameters.get("document_id")
        
        if not content:
            return {"error": "Content is required"}
        
        try:
            # Generate embedding for the document
            embedding = self.embedding_model.embed_document(content)
            
            # Add to Chroma
            self.chroma_client.add(
                documents=[content],
                embeddings=[embedding],
                metadatas=[metadata],
                ids=[document_id] if document_id else None
            )
            
            # Clear relevant cache entries
            cache_manager.clear_pattern("rag_query:*")
            
            logger.info(
                f"Added document to vector store",
                extra={"tool_name": "rag_tool", "document_id": document_id}
            )
            
            return {"success": True, "message": "Document added successfully"}
        except Exception as e:
            logger.error(
                f"Failed to add document to vector store: {str(e)}",
                extra={"tool_name": "rag_tool"}
            )
            return {"error": f"Failed to add document: {str(e)}"}
```

#### 3. Database Connection Pooling

```python
# backend/app/db/database.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import QueuePool
from .models import Base
from .core.config import settings
from .core.logging import logger

# Create database engine with connection pooling
engine = create_engine(
    settings.database_url,
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    pool_recycle=3600,
    echo=settings.debug
)

# Create session factory
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db() -> Session:
    """Dependency to get database session"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def init_db():
    """Initialize database with all tables"""
    try:
        Base.metadata.create_all(bind=engine)
        logger.info("Database initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize database: {str(e)}")
        raise

def check_db_health():
    """Check database health"""
    try:
        with engine.connect() as conn:
            conn.execute("SELECT 1")
        return True
    except Exception as e:
        logger.error(f"Database health check failed: {str(e)}")
        return False
```

### Sprint D: Documentation and Handoff

#### 1. One-Page Runbook

```markdown
# Customer Support AI Agent - Runbook

## System Overview
Customer Support AI Agent is a production-ready system consisting of:
- React frontend (TypeScript)
- Python FastAPI backend with Microsoft Agent Framework
- PostgreSQL database
- Redis cache
- Chroma vector database for RAG
- Prometheus/Grafana monitoring

## Quick Start Commands

### Deployment
```bash
# Deploy the system
./scripts/deploy.sh

# Check deployment status
./scripts/deploy.sh health

# View logs
./scripts/deploy.sh logs [service_name]
```

### Rollback
```bash
# Rollback to previous version
./scripts/rollback.sh

# Rollback to specific version
./scripts/rollback.sh to 1.0.0

# Restore from backup
./scripts/rollback.sh restore /path/to/backup
```

### Database Operations
```bash
# Run migrations
./scripts/deploy.sh migrate

# Create backup
docker-compose exec postgres pg_dump -U postgres customer_support > backup.sql

# Restore backup
docker-compose exec -T postgres psql -U postgres customer_support < backup.sql
```

## Health Checks
- Frontend: http://localhost:80/health
- Backend: http://localhost:8000/health
- Prometheus: http://localhost:9090/-/healthy
- Grafana: http://localhost:3000/api/health

## Monitoring
- Grafana Dashboard: http://localhost:3000
- Prometheus: http://localhost:9090
- AlertManager: http://localhost:9093

## Common Issues

### High Response Times
1. Check Grafana dashboard for bottlenecks
2. Verify Redis cache is working
3. Check database connection pool
4. Scale backend services if needed

### High Error Rates
1. Check application logs
2. Verify Agent Framework connectivity
3. Check Chroma vector database status
4. Review recent deployments

### Database Issues
1. Check connection pool settings
2. Monitor slow queries
3. Verify disk space
4. Check PostgreSQL logs

## Emergency Contacts
- DevOps Team: devops@example.com
- Development Team: dev@example.com
- On-call Engineer: +1-555-0123

## SLAs
- Response Time: 95th percentile < 2s
- Error Rate: < 1%
- Availability: > 99.5%
- Recovery Time: < 15 minutes
```

#### 2. Deployment Checklist

```markdown
# Deployment Checklist

## Pre-deployment
- [ ] Backup current database
- [ ] Backup Redis data
- [ ] Verify all tests passing
- [ ] Check security scan results
- [ ] Verify environment variables
- [ ] Check disk space availability
- [ ] Notify stakeholders of deployment

## Deployment
- [ ] Run deployment script
- [ ] Monitor service startup
- [ ] Verify health checks
- [ ] Check application logs
- [ ] Test core functionality
- [ ] Verify monitoring dashboards

## Post-deployment
- [ ] Run smoke tests
- [ ] Check error rates
- [ ] Monitor response times
- [ ] Verify alerting rules
- [ ] Update documentation
- [ ] Communicate deployment status

## Rollback Criteria
- Error rate > 5%
- Response time > 5s
- Health check failures
- Critical functionality broken
- User complaints received

## Rollback Steps
1. Stop current deployment
2. Run rollback script
3. Verify system health
4. Communicate rollback status
5. Investigate root cause
```

#### 3. Post-Deployment Validation

```python
# scripts/post_deployment_validation.py
import requests
import time
import sys
from typing import Dict, List

class PostDeploymentValidator:
    """Validates system health after deployment"""
    
    def __init__(self):
        self.base_url = "http://localhost:8000"
        self.frontend_url = "http://localhost:80"
        self.results = []
    
    def check_backend_health(self) -> bool:
        """Check backend health"""
        try:
            response = requests.get(f"{self.base_url}/health/", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return data.get("status") == "healthy"
            return False
        except Exception as e:
            print(f"Backend health check failed: {e}")
            return False
    
    def check_frontend_health(self) -> bool:
        """Check frontend health"""
        try:
            response = requests.get(f"{self.frontend_url}/health", timeout=10)
            return response.status_code == 200
        except Exception as e:
            print(f"Frontend health check failed: {e}")
            return False
    
    def test_chat_flow(self) -> bool:
        """Test basic chat flow"""
        try:
            # Create session
            response = requests.post(
                f"{self.base_url}/chat/sessions",
                json={"user_id": "test-user"},
                timeout=10
            )
            if response.status_code != 200:
                return False
            
            session_id = response.json()["session_id"]
            
            # Send message
            response = requests.post(
                f"{self.base_url}/chat/sessions/{session_id}/messages",
                data={"message": "Test message"},
                timeout=30
            )
            
            return response.status_code == 200
        except Exception as e:
            print(f"Chat flow test failed: {e}")
            return False
    
    def check_monitoring(self) -> bool:
        """Check monitoring systems"""
        try:
            # Check Prometheus
            response = requests.get("http://localhost:9090/-/healthy", timeout=10)
            if response.status_code != 200:
                return False
            
            # Check Grafana
            response = requests.get("http://localhost:3000/api/health", timeout=10)
            return response.status_code == 200
        except Exception as e:
            print(f"Monitoring check failed: {e}")
            return False
    
    def run_all_checks(self) -> Dict[str, bool]:
        """Run all validation checks"""
        checks = {
            "Backend Health": self.check_backend_health,
            "Frontend Health": self.check_frontend_health,
            "Chat Flow": self.test_chat_flow,
            "Monitoring": self.check_monitoring
        }
        
        results = {}
        for check_name, check_func in checks.items():
            print(f"Running {check_name}...")
            result = check_func()
            results[check_name] = result
            status = "âœ“" if result else "âœ—"
            print(f"{status} {check_name}: {'PASS' if result else 'FAIL'}")
        
        return results
    
    def generate_report(self, results: Dict[str, bool]) -> str:
        """Generate validation report"""
        passed = sum(results.values())
        total = len(results)
        
        report = f"""
Post-Deployment Validation Report
=================================
Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}
Overall Status: {'PASS' if passed == total else 'FAIL'}
Tests Passed: {passed}/{total}

Test Results:
"""
        
        for check_name, result in results.items():
            status = "PASS" if result else "FAIL"
            report += f"  {check_name}: {status}\n"
        
        if passed == total:
            report += "\nâœ“ All checks passed. Deployment validated successfully."
        else:
            report += "\nâœ— Some checks failed. Please investigate and resolve issues."
        
        return report

def main():
    validator = PostDeploymentValidator()
    results = validator.run_all_checks()
    report = validator.generate_report(results)
    
    print(report)
    
    # Exit with error code if any checks failed
    if not all(results.values()):
        sys.exit(1)

if __name__ == "__main__":
    main()
```

#### 4. Design Note Document

```markdown
# Design Decisions Document

## Overview
This document outlines the key design decisions made during the development of the Customer Support AI Agent system.

## Architecture Decisions

### 1. Microservices Architecture
**Decision**: Adopted microservices architecture with separate frontend, backend, database, and monitoring services.

**Rationale**:
- Allows independent scaling of components
- Improves fault isolation
- Enables technology diversity
- Simplifies deployment and maintenance

**Trade-offs**:
- Increased operational complexity
- Network latency between services
- More complex debugging

### 2. Technology Stack

#### Frontend: React with TypeScript
**Decision**: Used React with TypeScript for the frontend.

**Rationale**:
- Strong typing improves code quality
- Large ecosystem and community support
- Component-based architecture
- Good developer experience

**Trade-offs**:
- Steeper learning curve for TypeScript
- More verbose code compared to JavaScript

#### Backend: FastAPI with Python
**Decision**: Used FastAPI with Python for the backend.

**Rationale**:
- High performance with async support
- Automatic API documentation
- Type hints improve code quality
- Rich ecosystem for AI/ML libraries

**Trade-offs**:
- Global Interpreter Lock (GIL) limitations
- Performance compared to compiled languages

#### Database: PostgreSQL
**Decision**: Used PostgreSQL as the primary database.

**Rationale**:
- ACID compliance
- Rich feature set (JSON support, indexing)
- Strong consistency
- Good tooling and monitoring

**Trade-offs**:
- More complex than SQLite
- Requires more resources

#### Vector Database: Chroma
**Decision**: Used Chroma for vector storage and retrieval.

**Rationale**:
- Lightweight and easy to deploy
- Good integration with Python
- Supports metadata filtering
- Active development

**Trade-offs**:
- Less mature than enterprise solutions
- Limited scalability for very large datasets

### 3. AI Framework Integration

#### Microsoft Agent Framework
**Decision**: Integrated Microsoft Agent Framework for AI capabilities.

**Rationale**:
- Provides structured approach to AI agent development
- Built-in tool calling and context management
- Good documentation and examples
- Enterprise-ready features

**Trade-offs**:
- Vendor lock-in concerns
- Limited customization options
- Dependency on external service

#### RAG Implementation
**Decision**: Implemented Retrieval-Augmented Generation for knowledge base integration.

**Rationale**:
- Improves response accuracy
- Reduces hallucinations
- Provides source citations
- Easy to update knowledge

**Trade-offs**:
- Increased response latency
- Complexity in embedding management
- Requires good quality documents

### 4. Caching Strategy

#### Redis Integration
**Decision**: Implemented Redis for caching RAG queries and session data.

**Rationale**:
- Improves response times
- Reduces database load
- Supports various data structures
- Good performance

**Trade-offs**:
- Additional infrastructure component
- Memory usage considerations
- Cache invalidation complexity

### 5. Monitoring and Observability

#### Prometheus + Grafana Stack
**Decision**: Used Prometheus and Grafana for monitoring.

**Rationale**:
- Industry standard for monitoring
- Rich visualization capabilities
- Good alerting features
- Active community

**Trade-offs**:
- Steeper learning curve
- More infrastructure to maintain
- Configuration complexity

## Security Considerations

### 1. Authentication and Authorization
**Decision**: Implemented JWT-based authentication with role-based access control.

**Rationale**:
- Stateless authentication
- Widely adopted standard
- Good library support
- Scalable solution

### 2. Input Validation
**Decision**: Implemented comprehensive input validation and sanitization.

**Rationale**:
- Prevents injection attacks
- Ensures data integrity
- Improves error handling
- Compliance requirements

### 3. Rate Limiting
**Decision**: Implemented rate limiting to prevent abuse.

**Rationale**:
- Prevents DoS attacks
- Ensures fair usage
- Controls costs
- Improves stability

## Performance Optimizations

### 1. Connection Pooling
**Decision**: Implemented database connection pooling.

**Rationale**:
- Reduces connection overhead
- Improves throughput
- Better resource utilization
- Configurable limits

### 2. Async Processing
**Decision**: Used async/await for I/O operations.

**Rationale**:
- Improves concurrency
- Better resource utilization
- Reduced response times
- Scales better

### 3. Caching Layers
**Decision**: Implemented multiple caching layers.

**Rationale**:
- Reduces redundant computations
- Improves response times
- Decreases database load
- Better user experience

## Future Considerations

### 1. Scalability
- Consider container orchestration (Kubernetes)
- Implement horizontal auto-scaling
- Optimize database queries
- Consider read replicas

### 2. High Availability
- Implement multi-region deployment
- Add database failover
- Implement circuit breakers
- Add health checks

### 3. Security Enhancements
- Implement zero-trust architecture
- Add security scanning
- Implement audit logging
- Consider WAF integration

### 4. AI Improvements
- Fine-tune embedding models
- Implement context compression
- Add conversation analytics
- Consider multi-modal support

## Conclusion
The design decisions made prioritize reliability, maintainability, and scalability while balancing complexity and performance. The architecture allows for future growth and adaptation to changing requirements.
```

## Conclusion

Phase 4 implementation provides comprehensive deployment automation, monitoring dashboards, performance optimizations, and documentation. The implementation includes:

1. **Deployment Automation**: Production-ready Docker configurations, deployment scripts, and rollback procedures
2. **Monitoring Dashboards**: Comprehensive Prometheus/Grafana setup with alerting rules
3. **Performance Optimization**: Redis caching, connection pooling, and async processing
4. **Documentation**: Complete runbooks, deployment checklists, and design decisions

The system is now fully production-ready with all necessary components for deployment, monitoring, and maintenance. The comprehensive documentation and automation scripts ensure smooth deployment and operation in production environments.

The final phase will focus on final testing, security validation, and preparation for production handoff.

https://chat.z.ai/s/3516736b-1268-49e4-8f8f-59a19930d95e
